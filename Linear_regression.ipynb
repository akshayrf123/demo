{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_F19_solutions-2 (2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJz8ZxU7opMy"
      },
      "source": [
        "# Problem #1: Linear Regression\n",
        "\n",
        "##  Problem 1.1 (25 points)\n",
        "\n",
        "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
        "\n",
        "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
        "\n",
        "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1xPyXPoonO"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class linear_regression:\n",
        "  def __init__(self, learning_rate, iterations, \n",
        "               fit_intercept=True, normalize=False, coef=None):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.normalize = normalize\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.coef = coef\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "\n",
        "        \n",
        "    \"\"\"\n",
        "    Fit linear model.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training data\n",
        "    y : array_like, shape (n_samples, n_targets)\n",
        "        Target values.\n",
        "    \"\"\"\n",
        "    \n",
        "    features_array = np.array(X)\n",
        "    \n",
        "    global theta\n",
        "    \n",
        "    #final target values array \n",
        "    features_array1 = np.array(y)\n",
        "    \n",
        "    #length of the examples\n",
        "    m = len(features_array1)\n",
        "    number_of_ones = np.ones((m,1))\n",
        "    \n",
        "    #normalising the data\n",
        "    for i in range(0,len(features_array[0])):\n",
        "        features_array[:,i] = np.subtract(features_array[:,i],np.mean(features_array[:,i]))/np.std(features_array[:,i]) \n",
        "    \n",
        "    \n",
        "    #adding a extra coolumn for x0\n",
        "    features_array = np.concatenate([number_of_ones,features_array],axis = 1)\n",
        "    #there are 13 attributes, since we added a column to the data frame,we have to add 14*1 matrix suitable for mutiplication\n",
        "    number_of_features = len(features_array[0])\n",
        "    \n",
        "    \n",
        "    theta = np.zeros((number_of_features,1))\n",
        "    \n",
        "    L = []\n",
        "    #print(theta)\n",
        "    for i in range(self.iterations):\n",
        "        \n",
        "        #multiplying features_array with initial theta values to get the value of h\n",
        "        \n",
        "        h = np.dot( features_array,theta)\n",
        "        \n",
        "        \n",
        "        # subtracting the matrices to get the matrix which depicts the error\n",
        "        \n",
        "        error = np.subtract(h,features_array1)\n",
        "        \n",
        "        # mutiplying the obtained error matrix with the partial derivative term \n",
        "        \n",
        "        error = np.dot(features_array.T,error)\n",
        "    \n",
        "        #updating the theta values simultaneously\n",
        "        \n",
        "        theta = theta - ((self.learning_rate/m) * error)\n",
        "        \n",
        "    \n",
        "    #defining cost function\n",
        "    \n",
        "    \n",
        "    J_theta = (1/(2*m)) *  sum(np.square(np.subtract(np.dot(features_array,theta),features_array1)))\n",
        "    \n",
        "    print(\"Value of cost function:\",J_theta)\n",
        "    \n",
        "    \n",
        "    return theta,J_theta\n",
        "        \n",
        "    \n",
        "  def predict(self, X):\n",
        "    \"\"\"Predict using the linear model\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like, shape (n_samples, n_features)\n",
        "        Samples.\n",
        "    Returns\n",
        "    -------\n",
        "    C : array, shape (n_samples,)\n",
        "        Returns predicted values.\n",
        "    \"\"\"\n",
        "    testing_array = np.array(X)\n",
        "    \n",
        "    n = len(testing_array)\n",
        "    \n",
        "    #normalising the testing array \n",
        "    \n",
        "    for i in range(0,len(testing_array[0])):\n",
        "        \n",
        "        testing_array[:,i] = np.subtract(testing_array[:,i],np.mean(testing_array[:,i]))/np.std(testing_array[:,i])\n",
        "    \n",
        "    number_of_ones_testing = np.ones((n,1))\n",
        "    \n",
        "    testing_array = np.concatenate([number_of_ones_testing,testing_array],axis = 1)\n",
        "    \n",
        "    predicted_values = np.dot(testing_array,theta)\n",
        "    \n",
        "    return predicted_values\n",
        "    \n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfHPLIfVSXe3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Man3c1JrhVbr"
      },
      "source": [
        "## Problem 1.2 (10 points)\n",
        "\n",
        "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
        "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGFMD5yfSXe3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEFBL6WwhXUz",
        "outputId": "62f811b2-4db6-4568-e224-af7285bb7c45"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "boston = load_boston()\n",
        "features = pd.DataFrame(boston.data,columns = boston.feature_names)\n",
        "target = pd.DataFrame(boston.target, columns = ['target'])\n",
        "x_train,x_test,y_train,y_test = train_test_split(features,target,test_size = 0.3)\n",
        "creation = linear_regression(0.03,1000)\n",
        "a = creation.fit(x_train,y_train)\n",
        "print(\"Values of theta:\",a[0])\n",
        "print(\"alpha value is :\",creation.learning_rate)\n",
        "print(\"number of iterations is:\",creation.iterations)\n",
        "b = creation.predict(x_test)\n",
        "\n",
        "print(\"predicted values array\",b)\n",
        "\n",
        "\n",
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value of cost function: [11.95689281]\n",
            "Values of theta: [[22.93728814]\n",
            " [-0.79280092]\n",
            " [ 1.43942508]\n",
            " [ 0.45401705]\n",
            " [ 0.81052802]\n",
            " [-2.79130856]\n",
            " [ 2.47670631]\n",
            " [ 0.32801214]\n",
            " [-3.2802291 ]\n",
            " [ 3.09555183]\n",
            " [-1.80774431]\n",
            " [-1.98590708]\n",
            " [ 0.73877954]\n",
            " [-4.37114411]]\n",
            "alpha value is : 0.03\n",
            "number of iterations is: 1000\n",
            "predicted values array [[21.576032  ]\n",
            " [30.89522786]\n",
            " [23.24615531]\n",
            " [21.41655195]\n",
            " [32.6273438 ]\n",
            " [25.3697053 ]\n",
            " [33.22900521]\n",
            " [22.50860266]\n",
            " [14.2862538 ]\n",
            " [21.86446469]\n",
            " [14.04597272]\n",
            " [21.60255268]\n",
            " [19.98240662]\n",
            " [20.13995647]\n",
            " [18.25077801]\n",
            " [14.32637982]\n",
            " [28.84047332]\n",
            " [28.38720286]\n",
            " [36.18007089]\n",
            " [14.48178977]\n",
            " [25.01054986]\n",
            " [22.384186  ]\n",
            " [25.47404765]\n",
            " [29.51759419]\n",
            " [35.33940748]\n",
            " [34.14679403]\n",
            " [21.7875049 ]\n",
            " [ 9.16534139]\n",
            " [37.21908958]\n",
            " [28.04064948]\n",
            " [18.49420222]\n",
            " [19.54619246]\n",
            " [25.36236218]\n",
            " [20.00551392]\n",
            " [20.09526372]\n",
            " [19.3385253 ]\n",
            " [24.98841794]\n",
            " [18.12311076]\n",
            " [31.02924686]\n",
            " [30.17643122]\n",
            " [20.66617513]\n",
            " [31.69290129]\n",
            " [25.68315982]\n",
            " [16.86003398]\n",
            " [25.62685678]\n",
            " [26.11289524]\n",
            " [24.5805075 ]\n",
            " [ 8.16759461]\n",
            " [25.59158313]\n",
            " [34.85228538]\n",
            " [11.12860556]\n",
            " [27.88271315]\n",
            " [34.45978714]\n",
            " [23.59716586]\n",
            " [20.55611151]\n",
            " [21.24363993]\n",
            " [14.50674722]\n",
            " [18.18401356]\n",
            " [14.58211901]\n",
            " [33.42034987]\n",
            " [24.82831547]\n",
            " [12.87359808]\n",
            " [21.72784053]\n",
            " [15.58562145]\n",
            " [ 7.90920437]\n",
            " [22.36269275]\n",
            " [21.27407283]\n",
            " [20.22507544]\n",
            " [11.65967729]\n",
            " [38.69386861]\n",
            " [15.60847035]\n",
            " [27.84991613]\n",
            " [19.63667036]\n",
            " [24.72001955]\n",
            " [32.43269465]\n",
            " [19.28583122]\n",
            " [21.75684917]\n",
            " [28.84463408]\n",
            " [28.0553382 ]\n",
            " [21.17405015]\n",
            " [33.45809042]\n",
            " [17.23698281]\n",
            " [25.61586639]\n",
            " [22.53336489]\n",
            " [21.40766291]\n",
            " [17.60666659]\n",
            " [21.05585668]\n",
            " [10.76528399]\n",
            " [21.39625692]\n",
            " [18.45980805]\n",
            " [42.45399736]\n",
            " [20.23194291]\n",
            " [26.23139868]\n",
            " [17.66374025]\n",
            " [ 9.14160357]\n",
            " [ 9.26046671]\n",
            " [14.48765544]\n",
            " [14.46543104]\n",
            " [30.92837081]\n",
            " [20.21363179]\n",
            " [16.97876928]\n",
            " [23.83028416]\n",
            " [38.21460607]\n",
            " [17.65783618]\n",
            " [15.22926053]\n",
            " [39.07272984]\n",
            " [21.93044144]\n",
            " [22.45044856]\n",
            " [23.0503884 ]\n",
            " [29.75341303]\n",
            " [21.60382813]\n",
            " [14.83391426]\n",
            " [27.11689088]\n",
            " [23.54486167]\n",
            " [24.72436368]\n",
            " [22.36452045]\n",
            " [16.50242884]\n",
            " [22.60668064]\n",
            " [25.84593838]\n",
            " [22.48397613]\n",
            " [22.714624  ]\n",
            " [28.30690456]\n",
            " [36.22550809]\n",
            " [31.94811671]\n",
            " [28.12484068]\n",
            " [ 8.17655142]\n",
            " [27.81786238]\n",
            " [14.80845711]\n",
            " [20.2855905 ]\n",
            " [18.79086615]\n",
            " [26.40240322]\n",
            " [40.90656541]\n",
            " [17.5947618 ]\n",
            " [ 5.39148258]\n",
            " [17.26879028]\n",
            " [20.60456436]\n",
            " [-1.00336173]\n",
            " [25.31356267]\n",
            " [27.35783542]\n",
            " [28.32676103]\n",
            " [21.22176624]\n",
            " [ 9.85644064]\n",
            " [32.88178632]\n",
            " [11.63045335]\n",
            " [34.75016945]\n",
            " [19.23904016]\n",
            " [36.81129103]\n",
            " [39.46389579]\n",
            " [11.83068639]\n",
            " [39.49486292]\n",
            " [23.58316018]\n",
            " [21.53085551]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FXqtD_KhZwV"
      },
      "source": [
        "## Problem 1.3 (10 points)\n",
        "\n",
        "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFlcvY_piKus"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "boston = load_boston()\n",
        "\n",
        "features = pd.DataFrame(boston.data,columns = boston.feature_names)\n",
        "\n",
        "target = pd.DataFrame(boston.target, columns = ['target'])\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(features,target,test_size = 0.3)\n",
        "\n",
        "creation1 = linear_regression(0.03,1000)\n",
        "\n",
        "#first step try to generate all the 2 combinations features\n",
        "x_train_array = np.array(x_train)\n",
        "\n",
        "x_test_array = np.array(x_test)\n",
        "\n",
        "y_train_array = np.array(y_train)\n",
        "\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "L = []\n",
        "\n",
        "A = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12]) #creating an array of 13 elements since the number of features is 13\n",
        "\n",
        "#number of combinations to be considered\n",
        "\n",
        "B = [2,3,4,5,6,7,8,9,10,11,12]\n",
        "\n",
        "i = 0\n",
        "\n",
        "\n",
        "while i<11:\n",
        "    \n",
        "    feature_combinations = np.array(list(combinations(A,B[i])))\n",
        "    \n",
        "\n",
        "    for j in range(len(feature_combinations)):\n",
        "    \n",
        "        x_train_temp = np.delete(x_train_array,feature_combinations[j],axis = 1)\n",
        "    \n",
        "        a1 = creation1.fit(x_train_temp,y_train)\n",
        "    \n",
        "        L.append([a1[1],feature_combinations[j]])\n",
        "    \n",
        "        x_train_temp = np.copy(x_train_array)\n",
        "    \n",
        "        print(\"the cost function of deleting \", B[i] ,\"features is\",a1[1])\n",
        "        \n",
        "    i = i + 1\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "#identifying the minimum cost function and determining which features made it happen\n",
        "    \n",
        "list_array = np.array(L)\n",
        "    \n",
        "min_value = min(list_array[:,0])\n",
        "    \n",
        "print(\"the minimum cost function is,\", min_value)\n",
        "\n",
        "item = numpy.where(list_array == min_value)\n",
        "\n",
        "print(\"this is the tuple which identifies that set of variables which on deleting gives a minimum cost function\",item)\n",
        "\n",
        "\n",
        "#print(\"...............................................................................................\")\n",
        "    \n",
        "#similarly we have to repeat the procedure for 3 combinations, 4 combinations, 5 combinations and so on\n",
        "\n",
        "#three_feature_combinations = np.array(list(combinations(A,B[1])))\n",
        "\n",
        "#for i in range(len(three_feature_combinations)):\n",
        "    \n",
        "    #x_train_temp = np.delete(x_train_array, three_feature_combinations[i],axis = 1)\n",
        "    \n",
        "    #a1 = creation1.fit(x_train_temp,y_train)\n",
        "    \n",
        "    #L.append([a1[1],three_feature_combinations[i]])\n",
        "    \n",
        "    #print(\"the cost function on deleting 3 variables is \",three_feature_combinations[i],\" is \",a1[1])\n",
        "    \n",
        "    #x_train_temp = np.copy(x_train)\n",
        "    \n",
        "#print(\"..........................................................................\")\n",
        "    \n",
        "#four_feature_combinations = np.array(list(combinations(A,B[3])))\n",
        "\n",
        "#for i in range(len(four_feature_combinations)):\n",
        "    \n",
        "    #x_train_temp = np.delete(x_train_array,four_feature_combinations[i],axis = 1)\n",
        "    \n",
        "    #a1 = creation1.fit(x_train_temp,y_train)\n",
        "    \n",
        "   # L.append([a1[1],four_feature_combinations[i]])\n",
        "    \n",
        "    #print(\"the cost function on deleting 4 variables is \",four_feature_combinations[i],\" is \",a1[1])\n",
        "    \n",
        "    #x_train_temp = np.copy(x_train)\n",
        "\n",
        "\n",
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJSot41BkMrB"
      },
      "source": [
        "# Problem 2: Binary Classification\n",
        "\n",
        "## Problem 2.1 (5 points)\n",
        "\n",
        "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
        "\n",
        "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
        "\n",
        "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g96jFpGyMIFu",
        "outputId": "00c49054-be9e-440b-a945-e6675db45387"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import *\n",
        "import numpy as np\n",
        "\n",
        "def generate_dataset(k0, k1):\n",
        "    \n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    k0 : integer, number of samples for class 0\n",
        "    k1 : integer, number of samples for class 1\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
        "    samples and d is the number of features \n",
        "\n",
        "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
        "    \"\"\"\n",
        "    \n",
        "    #generating the 3 numbers required randomly\n",
        "    \n",
        "    a = randint(-10,10) \n",
        "    print(\"Value of coefficient a is\",a)\n",
        "    b = randint(-10,10)\n",
        "    print(\"Value of coefficient b is \",b)\n",
        "    c = randint(-10,10)\n",
        "    print(\" value of coefficient c is \",c)\n",
        "    L = []\n",
        "    L1 = []\n",
        "    \n",
        "    for i in range(100):\n",
        "    \n",
        "        x = randint(-10,10)\n",
        "        y = randint(-10,10)\n",
        "        if a * x + b *  y + c < 0 :\n",
        "            L.append([x,y])\n",
        "            plt.scatter(x,y,c = 'b')\n",
        "        else:\n",
        "            L1.append([x,y])\n",
        "\n",
        "    for i in range(100):\n",
        "    \n",
        "        x = randint(-10,10)\n",
        "        \n",
        "        y = randint(-10,10)\n",
        "        \n",
        "        if a * x + b *  y + c > 0 :\n",
        "            \n",
        "            L1.append([x,y])\n",
        "            \n",
        "            plt.scatter(x,y,c = 'g')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            L.append([x,y])\n",
        "            \n",
        "    straight_line = np.linspace(-10,10,10)\n",
        "    \n",
        "    straight_line_updated = (a*straight_line+c)/(-1*b)\n",
        "    \n",
        "    plt.plot(straight_line,straight_line_updated,'-r')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    pass\n",
        "\n",
        "generate_dataset(100,100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value of coefficient a is -1\n",
            "Value of coefficient b is  2\n",
            " value of coefficient c is  -5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mw15CFikXI1"
      },
      "source": [
        "## Problem 2.2 (35 points)\n",
        "\n",
        "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
        "\n",
        "\n",
        "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
        "\n",
        "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
        "\n",
        "1. Randomly choose a ${w}$ to begin with.\n",
        "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
        "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
        "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
        "    and go back to the previous step (of randomly chosing a sample)\n",
        "    \n",
        "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
        "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
        "        - $y$ is the class label for the sample under consideration\n",
        "        - $\\mathbf{x}$ is the data-point under consideration\n",
        "    \n",
        "    \n",
        "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPk7AZaLkXSh",
        "outputId": "6f1b3f2b-468f-43a6-d69e-d312f6ef7ac1"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "def fit_line(X, y):\n",
        "    \n",
        "    \"\"\"Predict using the binary classification model. Use the dataset generated \n",
        "    using generate_data() as input for this function.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like, shape (n_samples, n_features)\n",
        "        Samples.\n",
        "    y : array_like, shape (n_labels, 1)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    w : array, shape (1,n_features)\n",
        "        Returns the final weight vector w.  \n",
        "    \"\"\"\n",
        "    #using X.shape to determine the number of rows and columns\n",
        "    \n",
        "    m, d = X.shape \n",
        "    \n",
        "    #adding column of 1's to the X data\n",
        "    \n",
        "    number_of_ones = np.ones((m,1))\n",
        "    \n",
        "    X = np.concatenate([number_of_ones,X],axis = 1)\n",
        "    \n",
        "    #randomnly choosing the value\n",
        "    w = np.random.randint(-10,10,(1,d+1))\n",
        "    iters = 1000\n",
        "    \n",
        "    L1 = []\n",
        "    \n",
        "    for _ in range(iters):\n",
        "        \n",
        "        #To randomly take a sample without replacement\n",
        "        \n",
        "        idx = np.random.randint(m)\n",
        "        \n",
        "        if idx in L1:\n",
        "            \n",
        "            idx = np.random.randint(m)\n",
        "        \n",
        "        L1.append(idx)\n",
        "        \n",
        "        x_rows = X[idx,:]\n",
        "        #extracting the labels vector\n",
        "        y_labels = y[idx]\n",
        "        w_ = w.T\n",
        "        Y = np.dot(X[idx,:],w_)\n",
        "        if Y>=0 and y_labels ==-1 or Y<0 and y_labels ==1:\n",
        "            \n",
        "            w = w + y_labels * x_rows\n",
        "    \n",
        "    return w\n",
        "    \n",
        "    pass\n",
        "\n",
        "fit_line(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-13.  22. -14.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHc3t4e9MOxu"
      },
      "source": [
        "### Problem 2.3 (10 points)\n",
        "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
        "\n",
        "  - Answer: The above algorithm converges for a linearly seperable data for the fact that as we update the weights value for missclassified points and run it through a iteration. The weights of the mis-classified points gets updated itself and it tends to converge towards the straight line to pass on to the opposite side of the line. converging refers to the fact that the cost function values tend to move towards the mminimum of the curve in order to reduce the cost function. In other words the output tends to get closer and closer to a particular point. Sometimes the algorithm may diverge, where the ouput might move away and away from a particular value. In above the mis-classified points  tends to move towards the line when its weight are updated and its converging\n",
        "\n",
        "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
        "\n",
        "  - Answer: When the data is not linearly seperable the points cannot be classified into different classes using a straight line. That means a straight line is not sufficient enough to divide the samples into 2 different classes and the samples doesnt get classified properly by that straight line. When the data is not linearly seperable, We can salvage the situation as follows: We can map the original function to a higher dimensional function and  then involves finding a linearly seperable hyperplane in the new space. We can convert the orginal data into a quadratic or higher degree polynomial data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNfN1LSSXe8"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp4BX9-4SXe9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}